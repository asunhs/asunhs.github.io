---
layout: post
title: "성능 테스트를 위한 격리"
published: false
tags: [test, hoverfly]
---

# 성능 테스트와 격리

성능 테스트는 실제 부하를 받는 환경과 동일한 환경에서 이뤄질수록 의미가 높습니다.
하지만 정말 운영환경과 동일한 환경을 여러 목적을 위해 유지하는 것은 팀이나 기업의 사정에 따라 쉬운일은 아닙니다.
그렇다고 운영 중인 환경에 영향을 미칠수 있는 환경에서 성능을 테스트해 보는 것은 아주 위험천만한 일입니다.
만약, 성능을 확인하기 위해 운영환경과 아주 동일한 환경을 마련하기가 어렵다면 우리는 어떤 방법을 쓸 수 있을까요?

## 미니어처

성능 테스트는 반복 수행해보기에는 부담스럽기는 합니다. 하지만 성능 테스트를 자주 수행할 수 있고 그 부담을 줄일 수 있다면 개발팀에는 든든한 도구임에는 분명합니다.
따라서, 성능 테스트를 반복 수행할 수 있는 틀을 고민해봅니다. 영화 기법 중에 거대한 자연재해나 경관을 표현하기 위해서 미니어처 세트를 이용한 기법이 있습니다.
이 방법은 비용도 줄일 수 있지만 실제에서는 확인하기 힘든 장면이나 촬영방식도 진행 가능케 도와주기도 합니다.

![미니어처 기법](/images/posts/miniature.png)

성능 테스트에도 동일한 기법을 사용할 수 있을 것입니다. 하지만 현실을 축소하는데에는 모든 요소가 선형적으로 줄어들지는 않습니다.
어떤 요소는 로그 그래프를 따라 줄어들고 어떤 요소는 지수 그래프를 따라 줄어들기도 하며, 시간과 같은 요소는 설계에 따라 전혀 줄어들지 않을 수도 있습니다.

하지만 기본적으로 현실을 정밀하게 축소할 수 있다면 부담이 적은 테스트를 수행해볼 수 있음은 분명합니다.
그렇다면 어떻게 현실 축소의 정밀함을 얻을 수 있을까요? 여러 산술적인 비례를 계산하거나 할 수 있습니다. 예를들어 CPU 부하나 메모리 사용량 등은 부하 크기에 선형적으로 대응할 것이라 추측해볼 수 있습니다.
하지만, 사실 몇 분은 이미 고개를 갸우뚱하고 있을 듯 싶네요. 이러한 지표들도 실상은 비지니스마다 그 양상은 다를 수 밖에 없으며 예측하기에는 너무 복합적입니다.

따라서, 우리는 두 가지 방법을 사용할 수 있습니다.

- 가능하다면 운영과 동일한 스펙을 사용한다.
- 축소가 필요한 스펙이라면 실험적인 데이터를 수집하자.

사실 여유가 가능하다면 스펙을 축소하지 않는 것이 가장 예측도 쉽고 편합니다. 따라서, 몇몇 요소들은 기왕이면 운영과 동일한 스펙을 기준삼을 수 있는지 검토해 보았습니다.
대표적으로 "시간"들은 운영과 동일할 수 있도록 고려하였습니다. 예를 들어 처리 시간을 알고 싶다던가 지연 시간을 확인하고 싶다면 "시간"을 산술적으로 변환해야 하는 설계는 피하려고 하였습니다.

### 단순한 예측모델

성능의 특이점에 영향을 주는 요소는 보통 다음과 같습니다.

- 어플리케이션 처리 능력
- 병목 지점인 데이터베이스의 동시처리 능력

따라서, 이 두 요소를 모두 변수로 두고 수식을 세우면 식은 연립방정식을 풀어야 답을 낼 수 있습니다.
하나만 남기고 상수로 바꿀 수 있다면 식은 훨씬 쉬운 형태를 띌 수 있습니다.
이 요소들을 어떻게 다루는 것이 가장 편할까요?

어떤 자원의 동시처리 능력은 보통 그 규모와 로그 그래프적인 상관관계를 보이기도 합니다.
따라서, 어플리케이션의 스펙과 데이터베이스의 스펙을 모두 변수로 사용하면 꽤 힘든 산술 변환이 필요합니다.

어플리케이션 처리 능력은 대부분의 경우 투입 자원과 선형적인 상관관계를 보입니다.
반대로 데이터베이스의 스펙 대비 성능은 선형적이지 않습니다. 게다가 데이터베이스는 프로비져닝이 까다로우며 운영에서 사용하는 데이터베이스의 확장성도 높지 않곤 합니다.
따라서, 성능 테스트 환경에서도 데이터베이스의 스펙을 고정하고 대신 운영과 스펙이 아닌 원하는 성능지표가 어떤 비례를 보이고 있는지를 실험적으로 확인합니다.

이번엔 어플리케이션 처리 능력입니다. 어플리케이션 처리 능력은 (k8s 같은 리소스 매니징 환경 위에서) 다음과 같은 요소를 구분해볼 수 있습니다.

- 하나의 pod 에서 (지속적으로 혹은 순간적으로) 가능한 최대 처리량
- 투입 pod 수

우선적으로는 하나의 pod 의 스펙은 운영과 동일하게 유지하는 것이 가장 덜 부담스럽기 때문에 이 요소는 운영과 동일하게 유지하여 변수를 제거합니다.
그러면 투입 pod 수는 대부분 성능에 선형적인 관계를 보입니다. 아니라고 하더라도 이제 유일한 변수는 투입 pod 수 뿐입니다.

정리하자면 다음과 같습니다.

- 데이터베이스는 축소한 특정 성능지점으로 고정한다. (동일하고 단순한 산술변환으로 운영 성능 영향 도출 가능)
- 하나의 pod 에 성능은 운영과 동일하게 고정한다.
- 투입 pod 수를 조정하여 원하는 성능에 필요한 리소스를 테스트 한다.

이 방식에 쓸 최종 산술변환식은 꽤 단순합니다.

```
운영 pod 수 = 성능 상수 * 투입 pod 수
```

성능 상수는 데이터베이스 성능차이를 포함하여 복합적인 요소일 수 있으므로 경험적으로 테스트와 운영을 반복해서 관찰하여 더 정확한 성능 상수를 확인해가면 좋습니다.

이 과정은 미니어처 환경에 대한 예시입니다. 주요한 점은 가능한 운영과 동일하게 할 수 있는 요소, 그렇지 않다면 상수로 고정할 수 있도록 환경을 구축하는 것 입니다.
k8s 같은 환경은 이런 구성에 어플리케이션 리소스 측면의 편의를 제공합니다.

## 격리

성능 테스트의 한 가지 곤란한 점은 그 영향력 입니다. 다른 서비스나 테스트에 영향을 줄만한 부하는 모두를 곤란하게 합니다.
따라서, 격리가 가능하다면 테스트 영역을 격리하는 것이 좋습니다.

가장 좋은 격리는 전체 시스템을 테스트를 위해 한 벌 준비하는 것이지만 시스템이 커질 수록 이는 부담입니다.
약간 현실적인 얘기를 하자면, 현실에서는 모든 팀이 테스트 용이한 배포본을 유지하지 않을 수도 있습니다.
이런 경우에도 격리가 가능할까요?

차선으로 선택할 수 있는 방식은 mock service 를 활용하는 것 입니다.
mock service 는 기능에 따라 해당 서비스의 latency 도 모사할 수 있습니다.

이러한 mock service 를 만드는 과정에 필요한 것은 모사할 api 호출들을 마련하는 것 입니다.
몇몇 api 요청만을 모사하면 된다면 모르겠지만 MSA 를 지향할 수록, 또 도메인 컴포넌트 수가 5개 이상 늘어날 수록 비지니스에 사용하는 api 요청은 점점 늘어납니다.
게다가 성능 테스트는 복합 시나리오를 테스트 하기도 합니다. 실 운영과 유사하게 다수의 사용자로 부터 다양한 유즈케이스가 동시에 기능하는 시나리오 입니다.
이런 시나리오를 위해서 모사할 api 요청은 사실 직접 마련하기에는 너무 고달픈 작업입니다.

### Hoverfly

우리가 사용한 솔루션은 [hoverfly](https://hoverfly.io/) 입니다. hoverfly 는 일종의 proxy 로 이를 지나는 api 요청들을 기록할 수 있습니다.
기록한 내용을 토대로 시뮬레이션도 가능합니다.
또한 JUnit 과 결합하여 사용하는 [Hoverfly Java](https://docs.hoverfly.io/projects/hoverfly-java/en/latest/) 도 지원합니다.

hoverfly 를 사용하여 이제 각 컴포넌트들의 요청을 기록해 봅시다. hoverfly 를 셋업하는 것은 길게 설명하지 않겠습니다. dockerhub 의 [spectolabs/hoverfly](https://hub.docker.com/r/spectolabs/hoverfly) 이미지를 그대로 쓰면 충분합니다.
이렇게 standalone 모드로 동작하는 hoverfly 는 [REST API](https://docs.hoverfly.io/en/latest/pages/reference/api/api.html) 를 지원합니다.
그리고 admin 포트를 통한 dashboard web service 도 있으므로 브라우져로 접속하여 사용할 수도 있습니다.

### Proxying

hoverfly 가 녹화하는 요청은 destination 또한 조건에 포함합니다. hoverfly 는 일종의 middle man 이기 때문입니다. 따라서, 다음과 같은 Http 클라이언트 설정을 합니다.
feignClient 을 사용하여 proxy 설정을 하는 예제입니다.

```java
@Bean
@ConditionalOnProperty(prefix = "hoverfly", name = "enabled", havingValue = "true")
public Client feignHoverflyClient(
    @Value("${hoverfly.host}") String host,
    @Value("${hoverfly.port:80}") int port
) {
  Proxy proxy = new Proxy(Proxy.Type.HTTP, new InetSocketAddress(host, port));
  return new Client.Proxied(null, null, proxy);
}
```

이를 통해 녹화모드에서는 hoverfly 서버를 향하되 요청 정보를 변경하지 않고 테스트를 수행합니다.
성능테스트를 수행할 격리 내에 모든 서비스의 호출에 이 설정을 해두면 그야말로 경계 외로 통하는 모든 요청을 녹화할 수 있습니다.
녹화 자체는 성능을 테스트할 용도는 아니기 때문에 천천히 올바르게 녹화합니다.

### Capture

hoverfly dashboard 로 들어가서 capture 모드를 설정합니다.

![캡쳐 모드](/images/posts/hoverfly-dashboard-capture-mode.png)

이제 이 hoverfly 서버를 경유하는 요청과 응답 쌍을 녹화합니다. 이렇게 녹화한 결과는 `GET /api/v2/simulation` 요청으로 확인할 수 있습니다.

이 방식의 장점은 또한 이렇게 하나의 요청으로 유발한 네트워크 내역을 묶어서 확인할 수 있다는 점입니다.
이어서 이 기록들을 실제 테스트에 사용할 때, 각 시나리오에 맞는 네트워크 내역들을 찾기에 아주 좋은 분류로 도움을 받았습니다. 

## 시뮬레이션

이제 테스트를 수행해봅시다. hoverfly dashboard 에서 simulate 모드로 변경하거나 기록한 파일을 import 하여 hoverfly 를 재기동하면 테스트 환경으로 활용할 수 있습니다.
하지만 몇 가지 좀 더 수월한 테스트 조력을 원하였습니다.

- hoverfly 는 아주 디테일하게 기록하지만 실제 테스트에 적합한 동작은 덜 디테일하거나 추가적인 디테일이 필요할 수 있다.
- mocking 하는 service 의 특징을 모사하려고 할 때, 각 config 가 service 단위로 관리할 수 있는게 편했다.
- hoverfly 는 가볍고 빠르다고 하지만 실제 성능 테스트에 필요한 부하 환경에서 쓰기엔 부족했다.

이 점들을 하나씩 살펴보겠습니다.

### 테스트에 적합한 동작

mock service 는 기본적으로 요청에 반응하는 응답을 바랍니다. hoverfly 역시 요청-응답 쌍으로 캡쳐하므로 mock server 로써 역할을 충분히 수행할 수 있습니다.
여기서 꽤 자주 마주하는 상황이 있습니다. 보통 매 요청마다 새로 생성하는 값들입니다.
만약 요청에 timestamp 성격의 데이터나 혹은 요청번호 같은 sequence 성격의 데이터가 있다면 단순한 캡쳐 결과로는 예상하는 응답을 매칭해주지 않을 것 입니다.
기록했던 요청의 데이터와 해당 부분이 다르기 때문이죠. 이런 데이터는 매칭에서 가볍게 무시해주면 됩니다.

물론, hoverfly 가 제공하는 simulation 문법에 이럴 경우를 위한 매칭 문법들을 제공합니다. 하지만, 말하자면 capture 방식에서는 이를 아주 멋지게 추론해 주지는 않습니다.
capture 결과를 바탕으로 필요한 매칭 문법으로의 수정을 약간 해야합니다.

예를 들어, body 에 대한 기본 결과가 다음과 같을 때,

```json
{
  "matcher": "exact",
  "value": "{\"requestKey\":\"1000\",\"itemNo\":\"10000001\",\"sellerNo\":\"1000001\",\"qty\":1,\"price\":10000}"
}
```

이 요청은 부하 시, 매 요청마다 바뀌는 requestKey 를 대응하지 못합니다. 이 경우는 다음 방식으로 대응할 수 있습니다.

```json
{
  "matcher": "jsonpartial",
  "value": "{\"itemNo\":\"10000001\",\"sellerNo\":\"1000001\",\"qty\":1,\"price\":10000}"
}
```

이제 requestKey 에 대한 매칭은 무시하고 요청의 내용이 동일하면 원하는 응답으로 대응할 수 있습니다.

그런데 해당 요청의 응답 모양은 다음과 같았습니다.

```json
{
  "status": 200,
  "body": "{\"requestKey\":\"1000\",\"itemNo\":\"10000001\",\"fee\":50}"
}
```

요청 시 requestKey 는 무시하였지만 요청에서 들어온 값과 상관없이 응답에서는 해당 값으로 1000 을 보낼 것입니다. 이 또한 hoverfly 에서 제공하는 문법이 있습니다.

```json
{
  "status": 200,
  "body": "{\"requestKey\":{{Request.Body 'jsonpath' '$.[0].requestKey'}},\"itemNo\":\"10000001\",\"fee\":50}"
}
```

이렇게 request 와 response 의 스펙을 변경하면 우리가 원하는 방식으로 동작합니다.
이 작업은 사실 충분히 편한 도구로 대체 가능한 작업입니다. 따라서, 단순한 설정을 통해 원하는 문법 변경을 적용하는 도구를 활용합니다.

```json
{
  "ignoreProperties": [
    {
      "path": "/v1/fee",
      "position": "requestKey"
    }
  ],
  "templatedProperties": [
    {
      "path": "/v1/fee",
      "position": "data[0].requestKey",
      "from": "[0].requestKey"
    }
  ]
}
```

이런식으로 hoverfly 가 남겨준 아주 충분한 기록을 바탕으로 테스트에 적합한 행위를 적용해야 하는데 이 과정 또한 도구를 개발하여 사용합니다.
제가 사용한 도구는 json 편집에 용이한 javascript 를 돌릴 수 있는 node.js 어플리케이션으로 개발하여 사용하였습니다.
이를 통해 매번 동일한 요구사항을 담은 문법 변경을 자동으로 수행하도록 하면 테스트 시나리오를 다양하게 준비하는 과정이 훨씬 수월해집니다.

### 서비스의 특징

캡쳐 시에는 하나의 hoverfly 로도 요구사항이 충분합니다.
hoverfly 는 destination 도 꼼꼼히 기록해주기 때문에 하나의 요청으로 발생한 모든 네트워크 내역을 모두 하나의 hoverfly proxy 에 흘려 한꺼번에 기록하는 편이 관리가 편하기도 합니다.

하지만 simulation 은 하나의 hoverfly 에서 할 필요는 없습니다. 특히나 각 서비스의 특징들을 적용하려고 할 때는 또 다른 관점이 더 편합니다.

예를들어 네트워크 내역은 서비스를 막론하고 테스트 시나리오 별로 모아두는 편이 편하지만 특정 서비스의 응답시간을 적용하는 것은 테스트 시나리오보다는 해당 서비스를 기준으로 설정을 관리하는 것이 편합니다.
hoverfly 는 좀 더 세세한 `delay` 에 관한 설정도 가지고 있지만, 다음과 같이 `globalAction` 을 설정할 수도 있는데 이는 hoverfly 시뮬레이션 전체에 적용합니다.

```json
{
  "globalActions": {
    "delays": [
      {
        "urlPattern": ".",
        "delay": 15
      }
    ],
    "delaysLogNormal": []
  }
}
```

만약, 시뮬레이션을 위한 hoverfly 를 서비스 별로 준비한다면 `globalAction` 설정을 통해 서비스의 특징을 적용할 수 있습니다.
이 때, 각 시뮬레이션을 위한 hoverfly 는 모든 네트워크 기록을 들고 올라갈 필요가 없습니다.
따라서, 테스트 시나리오 기록 중 해당 서비스 도메인과 관련한 요청-응답 기록만 필터링하면서 `globalAction` 을 적용하도록 도구를 사용하기로 하였습니다.

이 도구는 위의 문법 변경을 위한 도구와 협력할 수 있습니다. 실제로도 이 두 도구를 협력하여 좀 더 매끄러운 테스트 준비과정을 마련하였습니다.

### 테스트에 충분한 부하를 견디기

지금까지의 준비를 토대로 실제 부하 테스트를 진행하는 과정에서 사실상 hoverfly 가 성능 테스트를 도울만한 도구는 아니라는 걸 확인하였습니다.
우리는 테스트 시나리오를 마련하는 과정에 테스트 경계 밖의 요청-응답을 기록하고 테스트 용이한 행위로 시뮬레이션을 적용하는 것이 큰 도움을 받았지만 그것으로는 부족하였습니다.

결국 부하를 충분히 받을 수 있는 mock server 어플리케이션을 따로 도입하기로 하였습니다.
대신, 앞서 마련한 hoverfly 문법을 지원하여 지금까지 활용한 방법과 도구들을 그대로 재사용할 수 있는 환경을 원했습니다.

지금까지의 도구들은 node.js 로 만들었으며 역시 지연시간을 흉내내면서 부하를 견디기에는 node.js 만한 방식이 없었습니다.
reactor 패턴으로 non-blocking I/O 를 사용하기 때문에 지연시간을 흉내낼 때 전혀 CPU 에 부담을 주지 않기 때문입니다.

물론 역시, 위에 언급한 도구들과의 협력 또한 매끄러웠습니다.
아예 mock server 어플리케이션에 이 도구들을 포함하여 캡쳐 원본과 서비스 설정을 전달하면 대상 서비스를 특정한 mock server 로 기동할 수 있도록 작성하였습니다.

node.js 를 기반으로 한 mock server 는 아주 훌륭하게 부하를 받아주었으며, hoverfly 도구들을 재사용한 방식은 여전히 매끄러운 테스트 준비과정을 제공하였습니다.



# 정리 