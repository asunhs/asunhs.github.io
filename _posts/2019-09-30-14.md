---
layout: post
title: "Probabilistic Data Structures"
published: true
tags: [probabilistic, ds]
---

# Data Structures

이전에 Persistent Data Structure 를 다루면서 재미를 느낀 이유는 불변성을 훨씬 경제적인 비용으로 획득하기 위한 자료구조를 보면서 우리가 쉽게 접하는 자료구조 외에도 우리의 목적과 의도를 도와줄 수 있는 방법을 제공하는 자료구조가 있다는 점이었습니다. 관련하여 관심이 높아진 경위로 팀원내에서 이러한 재밌는 자료구조들을 공유하고 정리해보는 시간을 갖게 되었습니다.

그 중 모두가 흥미있게 살펴본 것은 이른바 확률적 자료구조 였습니다. 확률이라는 수학적인 기초를 토대로 이뤄진 자료구조로 데이터를 다루는 접근법 자체가 다른 경험을 하게 되었습니다. 아무래도 엄밀하고 꼼꼼히 데이터를 다루는데 익숙한 우리에게 확률로 데이터를 다룬다? 는 점은 벌써 버그가 상상되는 일이었습니다. 하지만 확률또한 수학적으로 탄탄한 기법이듯이 확률적 기초로 이뤄진 이런 자료구조들을 알아본 후엔 그런건 괜한 걱정에 불과했다는 생각입니다.

<!-- more -->

# 확률적 자료구조

확률적 자료구조라는 카테고리에 속한 자료구조는 다음과 같은 종류들이 있습니다.

> - Bloom Filter
> - Count-min Sketch
> - Cuckoo Filter
> - HyperLogLog
> - Kinetic Hanger
> - Kinetic Heater
> - Locality-sensitive Hashing
> - MinHash
> - Quotient Filter
> - Random Binary Tree
> - Random Tree
> - Rapidly-exploring Random Tree
> - SimHash
> - Skip List
> - Treap

참고 [https://en.wikipedia.org/wiki/Category:Probabilistic_data_structures](https://en.wikipedia.org/wiki/Category:Probabilistic_data_structures)

 

이들은 모두 확률을 기반으로 특정 이점을 얻고 있는 자료구조인데, 크게는 두 가지 부류로 구분해 볼 수 있습니다.

확률적으로 평균적인 시간복잡도를 보장하는 자료구조, 다시 말해 낮은 확률로 매우 최악의 시간복잡도를 가질수도 있습니다.
정확한 값이 아니라 오차를 허용하는 근사값을 계산하는 대신 시간/공간 복잡도를 극소화시키는 자료구조
1 에 해당하는 자료구조들은 Skip List 나 Treap 등이 있습니다. 이 자료구조들은 데이터가 확률적이거나 하지 않습니다. 엄밀한 자료구조이지만 데이터의 접근/활용 등의 시간복잡도를 평균적으로 낮추기 위해 확률적인 접근을 합니다. 따라서, 이런 자료구조는 최악의 상황이 될 확률을 매우 낮추거나 백업 플랜을 두는 방식을 겸하기도 하면서 현실적인 매우 높은 성능을 보장합니다.

2 에 해당하는 자료구조들은 대표적으로 Bloom Filter, Count-min Sketch, HyperLogLog 등이 있습니다. 이 자료구조들은 데이터를 확률적으로 표현합니다. 하지만 우리의 일상엔 생각보다 근사값이면 충분한 요소들이 있습니다. 예를 들면, 중복 시청을 제외한 게시물의 총 조회 회원 수 등을 알고 싶다면 게시물(N) 마다 총 회원(M) 에 대한 Flag 를 유지해야 합니다. 우리 상품과 고객 수를 생각해본다면 이는 무시무시한 시간/공간복잡도를 요구하는 데이터일 것입니다. 만약 이를 단지 10MB 정도와 매우 빠른 시간안에 계산할 수 있는 대신 높은 기대수준의 근사값을 얻을 수 있다면, 우리는 충분히 고려해볼 수 있을 것입니다. 10,001명이 본 것이든 9,999명 본 것은 우리가 찾고자하는 의미에서는 큰 차이가 없기 때문입니다.

대신 이러한 자료구조들은 특정한 의미의 수치를 찾는데 특화되어 한번에 하나의 역할씩으로 확인되곤 합니다.

앞서 얘기한 중복을 제외한 항목의 고유 카운트를 세는 자료구조는 HyperLogLog 입니다. 이외에 다음과 같은 자료구조와의 연관이 있습니다.

| 자료구조 | 역할 |
|---|---|
| HyperLogLog | Cardinality |
| BloomFilter | Membership Query |
| Count-min Sketch | Frequency |

이외에도 앞서 소개한 만큼 다양한 자료구조들도 있지만 이번에 살펴본 대상은 이정도가 되었습니다.

# HyperLogLog

HyperLogLog 는 예시로 소개한 것 처럼 유일한 원소의 개수를 추정하는 확률적 자료구조 입니다. Redis 에서 지원하는 자료구조에도 포함될 정도로 대표적인 확률적 자료구조로 알려져 있습니다. 다음은 셰익스피어의 전 작업에 등장한 단어별 수를 세는 방식을 각기 다른 세 개의 자료구조를 통해 진행한 결과입니다.

![HyperLogLog](/images/posts/big-data-counting.png)

참고 [http://highscalability.com/blog/2012/4/5/big-data-counting-how-to-count-a-billion-distinct-objects-us.html](http://highscalability.com/blog/2012/4/5/big-data-counting-how-to-count-a-billion-distinct-objects-us.html)

어떤 시스템은 단 1%의 Error 가 용인되지 않을 수도 있습니다. 하지만 어떤 경우에는 Bytes Used 가 10,447,016 bytes 에서 512 bytes 로 줄어드는 점을 더 중요하게 고민할 수도 있습니다. 일상의 데이터를 노출하는 과정에 Humanize 를 통해 전달에 더 초점을 맞추기도 합니다. 67801 이 아니라 70K 로 표현되는 경우입니다. 이런 경우 사실 표현 자체에 정확도라는 근사 표현을 이미 수용하는 경우입니다. (앞선 예시가 그랬죠) 그렇다면 우리의 눈은 자연스레 Error 보다는 Bytes Used 에 옮겨올 것 입니다. 사실 위 실험은 조금 화려한 실험입니다. 3% 의 오차가 더 줄어들어야 의미가 있다면 HyperLogLog 는 충분히 Bytes Used 와의 트레이드 오프를 제공합니다. 더 많은 Bytes 를 연산에 포함하면 Error 는 줄어들고 최종적으로 HashSet 과 유사한 Bytes 를 사용하면 Error 를 0 으로 만들 수도 있습니다. 이 오차는 다음과 같습니다. 여기서 m 은 버켓 수인데 다시말해 사용할 Bytes 수와 비례합니다.

![버켓](/images/posts/0bf2803a3aff4ac13b057608e026fc32e97baecd.svg)

참고 [https://en.wikipedia.org/wiki/HyperLogLog](https://en.wikipedia.org/wiki/HyperLogLog)

HyperLogLog 의 구조는 생각보다는 간단했습니다.

![HyperLogLog 의 구조](/images/posts/1_-_rf75FXF4va8ofa9OEgdg.png)

참고 [https://blog.devartis.com/hyperloglogs-a-probabilistic-way-of-obtaining-a-sets-cardinality-3b5e6a982a12](https://blog.devartis.com/hyperloglogs-a-probabilistic-way-of-obtaining-a-sets-cardinality-3b5e6a982a12)

값을 upper bit 과 lower bit 으로 나눕니다. 마법은 lower bit 에 있습니다. 같은 upper bit (버켓) 를 갖는 값들은 모두 모아서 하나의 lower bit 에 뭉뚱그려 놓는 것입니다. 단지, 이들을 뭉뚱그리면서 lower bit 가 어떤 요소를 새로 포함시킬 때 그 여부를 확률적으로 표현되도록 합니다. 이 방식도 그리 이해가 어렵진 않습니다. leading zero 를 통해서 어떤 요소가 해당 lower bit 에 포함되어 있을 가능성을 확률로 봅니다. 예를 들어 이미 뭉뚱그려진 bit 가 01000 이라면 이는 00100 을 50% 의 확률로 00010 을 25% 의 확률로 포함하고 있음을 나타냅니다. 이 버켓에 10110 등으로 lower bit 으로 갖는 요소가 추가되면 이는 10000 으로 갱신됩니다. 그리고 각 요소에 대한 확률이 조금은 바뀝니다. 이 확률에 아직 이 사건이 일어나지 않았을 확률을 곱하면 우리가 원하는 해당 요소가 현재 존재할 확률을 알게 되고 다음과 같은 유도 공식을 통해 원하는 근사값을 확인하게 됩니다.

![bit](/images/posts/e68865b48f91a302624ca310c75f127ce64871ac.svg)

다행히 우리가 직접 이런 계산을 하기전에 [steam-lib (https://github.com/addthis/stream-lib/)](https://github.com/addthis/stream-lib/) 같은 라이브러리들이 있습니다.

# Bloom Filter

다음은 Membership Query 를 위한 자료구조 입니다.  어떤 원소가 해당 집합에 포함되어 있는지 여부를 판단하기 위한 자료구조입니다. 만약 스마일클럽 회원인지를 확인하고 싶은 경우가 빈번하게 발생한다고 한다면, 이 모든 요청은 스마일클럽 서비스가 받아준다면 큰 문제는 없습니다. 하지만 만약 여의치가 않다면 이 여부를 Cache Layer 등을 통해서 유지하고자 할 가능성이 있습니다. 이 때, 회원 수가 M 이라면 그만큼의 공간복잡도를 요구하게 됩니다. Bloom Filter 는 회원 여부에 대해서 근사값을 확인하면서 공간복잡도를 크게 낮출 수 있습니다.

여기까지 얘기를 듣고보면 의아할 수도 있습니다. 스마일크럽 회원인지 여부는 사실 근사값으로 대체가능하지 않을 정도의 높은 정확도를 요구할 것이기 때문입니다. 이 부분은 분명 사실이지만 Bloom Filter 의 재미난 특성이 한가지 있습니다. 그것은 False Negative 는 존재하지 않는 다는 점입니다. 이는 원소를 검사할 때 존재하지 않는 원소를 존재한다고 판단할 가능성은 없다는 뜻입니다. 즉, “어떤 원소가 포함될 수도 있다”와 “어떤 원소는 분명히 포함하지 않았다” 를 알 수 있는 것이죠.

이를 통해 해당 고객이 스마일클럽 회원이 아닌 경우는 확실히 판단을 내어주고 만약 회원이 맞다고 판단이 된 경우에는 이 신뢰를 높히기 위해 직접 스마일클럽 서비스에 요청해서 진실을 확인하는 2단계 전략을 취할 수 있습니다. 특히 소수의 멤버들을 확인하기 위해 다수의 멤버들로부터 Membership Query 를 받아야 한다면 다수의 요청에 대해 미리 멤버가 아님을 확인해 줄 수 있을 것입니다.

물론 HashSet 을 사용하면 당연히 100% 의 판단이 가능할테지만, 여기서의 강점은 또한 공간복잡도를 획기적으로 낮출 수 있다는 점입니다.

![Bloom_filter](/images/posts/540px-Bloom_filter_fp_probability.svg.png)

참고 [https://en.wikipedia.org/wiki/Bloom_filter](https://en.wikipedia.org/wiki/Bloom_filter)

만약 우리가 1% (0.01) 의 기대확률을 원한다면 회원 수가 1,000명의 모수일 때는 8KB, 1,00만명의 모수일 때는 32MB 만 있으면 됩니다.

Bloom Filter 또한 구조는 어렵지 않습니다. 서로다른 해시 함수를 여러번 적용한 값을 중첩해서 해당 값은 OR 연산으로 한 곳에 업데이트 하는 것 입니다.

![Bloom_filter](/images/posts/540px-Bloom_filter.svg.png)

참고 [https://en.wikipedia.org/wiki/Bloom_filter](https://en.wikipedia.org/wiki/Bloom_filter)

만약 검사를 원하는 원소를 같은 해시 함수들로 적용하여 확인한 값 중에 1이 하나라도 없으면 분명히 해당 원소는 포함된 적이 없는 것입니다. 모두 1이라면 해당 원소가 포함되었거나 확률적으로 다른 원소들의 결과에 뭉뚱그려져서 나타난 것일 수도 있습니다.

# 정리

대표적인 확률적 자료구조에 대하여 알아보는 시간이 되었습니다. 지난 시간 살폈던 Persistent Data Structure 와는 또 다른 매력으로 우리가 익히 알고 있는 자료구조 외에도 잘 활용하면 획기적일 수 있는 도움이 되는 자료구조들이 있다는 것에 신기하고 재밌는 시간이 되었습니다. 확률이라는 점에 조금 엄밀하지 못할 것이라는 걱정이 있었지만, 오히려 그점을 통찰하거나 보완해서 이득을 극대화하는 활용들의 사례들을 보면서, 우리가 가지고 있는 문제들도 혹시 이런 자료구조의 선행된 패턴들을 통해 해결되지 않을까 하는 생각을 해보게 됩니다. 여기까지 읽어주신 수고에 감사드립니다.

